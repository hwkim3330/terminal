#!/usr/bin/env python3
"""
ëª¨ë¸ ìºì‹œ ë° ìµœì í™” ì„¤ì • ìŠ¤í¬ë¦½íŠ¸
GPU ë©”ëª¨ë¦¬ ìµœì í™”, ëª¨ë¸ ì–‘ìí™”, ìºì‹œ ê´€ë¦¬
"""

import os
import json
import shutil
import subprocess
from pathlib import Path
import torch
import psutil

BASE_DIR = Path("/mnt/data/lfm_agi")
CACHE_DIR = BASE_DIR / "cache"
MODELS_DIR = BASE_DIR / "models"

def get_system_info():
    """ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘"""
    info = {
        "cpu_count": psutil.cpu_count(),
        "memory_gb": round(psutil.virtual_memory().total / (1024**3), 2),
        "disk_free_gb": round(shutil.disk_usage(BASE_DIR).free / (1024**3), 2),
        "cuda_available": torch.cuda.is_available()
    }
    
    if info["cuda_available"]:
        info["cuda_device_count"] = torch.cuda.device_count()
        info["cuda_devices"] = []
        for i in range(torch.cuda.device_count()):
            props = torch.cuda.get_device_properties(i)
            info["cuda_devices"].append({
                "id": i,
                "name": props.name,
                "memory_gb": round(props.total_memory / (1024**3), 2)
            })
    
    return info

def setup_environment_cache():
    """í™˜ê²½ ë³€ìˆ˜ ìºì‹œ ì„¤ì •"""
    print("ğŸ”§ í™˜ê²½ ë³€ìˆ˜ ìºì‹œ ì„¤ì •...")
    
    cache_dirs = {
        "HF_HOME": CACHE_DIR / "huggingface",
        "TRANSFORMERS_CACHE": CACHE_DIR / "transformers",
        "TORCH_HOME": CACHE_DIR / "torch",
        "TORCH_EXTENSIONS_DIR": CACHE_DIR / "torch_extensions",
        "CUDA_CACHE_PATH": CACHE_DIR / "cuda",
        "NUMBA_CACHE_DIR": CACHE_DIR / "numba",
        "OLLAMA_MODELS": BASE_DIR / "ollama_models"
    }
    
    # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
    for env_var, path in cache_dirs.items():
        path.mkdir(parents=True, exist_ok=True)
        os.environ[env_var] = str(path)
        print(f"âœ… {env_var} = {path}")
    
    # í™˜ê²½ ë³€ìˆ˜ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
    env_script = "#!/bin/bash\n# LFM AGI í™˜ê²½ ë³€ìˆ˜ ì„¤ì •\n\n"
    for env_var, path in cache_dirs.items():
        env_script += f"export {env_var}=\"{path}\"\n"
    
    env_script += """
# PyTorch ìµœì í™” ì„¤ì •
export OMP_NUM_THREADS=8
export MKL_NUM_THREADS=8
export TOKENIZERS_PARALLELISM=false

# CUDA ìµœì í™”
export CUDA_LAUNCH_BLOCKING=0
export CUDA_DEVICE_ORDER=PCI_BUS_ID

# ë©”ëª¨ë¦¬ ìµœì í™”
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
export CUDA_MEMORY_FRACTION=0.8

echo "âœ… LFM AGI í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ì™„ë£Œ"
"""
    
    with open(BASE_DIR / "setup_env.sh", "w") as f:
        f.write(env_script)
    
    os.chmod(BASE_DIR / "setup_env.sh", 0o755)
    print(f"âœ… í™˜ê²½ ì„¤ì • ìŠ¤í¬ë¦½íŠ¸: {BASE_DIR}/setup_env.sh")

def create_optimization_configs():
    """ëª¨ë¸ ìµœì í™” ì„¤ì • ìƒì„±"""
    print("âš¡ ëª¨ë¸ ìµœì í™” ì„¤ì • ìƒì„±...")
    
    # GPU ë©”ëª¨ë¦¬ ê¸°ë°˜ ìµœì í™” ì„¤ì •
    system_info = get_system_info()
    
    optimization_configs = {
        "low_memory": {
            "description": "ì €ë©”ëª¨ë¦¬ í™˜ê²½ (< 8GB GPU)",
            "model_config": {
                "load_in_8bit": True,
                "device_map": "auto",
                "low_cpu_mem_usage": True,
                "torch_dtype": "float16"
            },
            "generation_config": {
                "max_length": 512,
                "batch_size": 1,
                "use_cache": True
            }
        },
        "medium_memory": {
            "description": "ì¤‘ê°„ë©”ëª¨ë¦¬ í™˜ê²½ (8-16GB GPU)",
            "model_config": {
                "load_in_8bit": False,
                "device_map": "auto", 
                "low_cpu_mem_usage": True,
                "torch_dtype": "float16"
            },
            "generation_config": {
                "max_length": 1024,
                "batch_size": 2,
                "use_cache": True
            }
        },
        "high_memory": {
            "description": "ê³ ë©”ëª¨ë¦¬ í™˜ê²½ (> 16GB GPU)",
            "model_config": {
                "load_in_8bit": False,
                "device_map": None,
                "low_cpu_mem_usage": False,
                "torch_dtype": "float16"
            },
            "generation_config": {
                "max_length": 2048,
                "batch_size": 4,
                "use_cache": True
            }
        }
    }
    
    # ì‹œìŠ¤í…œì— ë§ëŠ” ê¸°ë³¸ ì„¤ì • ì„ íƒ
    if system_info["cuda_available"]:
        max_gpu_memory = max(device["memory_gb"] for device in system_info["cuda_devices"])
        if max_gpu_memory < 8:
            default_config = "low_memory"
        elif max_gpu_memory < 16:
            default_config = "medium_memory"
        else:
            default_config = "high_memory"
    else:
        default_config = "low_memory"
    
    optimization_configs["default"] = optimization_configs[default_config]
    optimization_configs["system_info"] = system_info
    
    with open(CACHE_DIR / "optimization_configs.json", "w") as f:
        json.dump(optimization_configs, f, indent=2)
    
    print(f"âœ… ìµœì í™” ì„¤ì • ìƒì„± ì™„ë£Œ: {default_config} ì„ íƒë¨")

def setup_model_quantization():
    """ëª¨ë¸ ì–‘ìí™” ì„¤ì •"""
    print("ğŸ”¢ ëª¨ë¸ ì–‘ìí™” ì„¤ì •...")
    
    quantization_script = """#!/usr/bin/env python3
'''
ëª¨ë¸ ì–‘ìí™” ìŠ¤í¬ë¦½íŠ¸
8bit, 4bit ì–‘ìí™”ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëŒ€í­ ê°ì†Œ
'''

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import json
from pathlib import Path

def quantize_model(model_name: str, output_dir: str, bits: int = 8):
    '''ëª¨ë¸ ì–‘ìí™” ë° ì €ì¥'''
    print(f"ğŸ”¢ {model_name} {bits}bit ì–‘ìí™” ì‹œì‘...")
    
    # ì–‘ìí™” ì„¤ì •
    if bits == 8:
        quantization_config = BitsAndBytesConfig(
            load_in_8bit=True,
            llm_int8_threshold=6.0,
            llm_int8_has_fp16_weight=False
        )
    elif bits == 4:
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True
        )
    else:
        raise ValueError("bitsëŠ” 4 ë˜ëŠ” 8ì´ì–´ì•¼ í•©ë‹ˆë‹¤")
    
    try:
        # ëª¨ë¸ ë¡œë“œ
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=quantization_config,
            device_map="auto",
            torch_dtype=torch.float16
        )
        
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        # ì €ì¥
        output_path = Path(output_dir) / f"{model_name.replace('/', '_')}_{bits}bit"
        output_path.mkdir(parents=True, exist_ok=True)
        
        model.save_pretrained(output_path)
        tokenizer.save_pretrained(output_path)
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata = {
            "original_model": model_name,
            "quantization_bits": bits,
            "quantization_type": "bitsandbytes",
            "torch_dtype": "float16",
            "device_map": "auto"
        }
        
        with open(output_path / "quantization_metadata.json", "w") as f:
            json.dump(metadata, f, indent=2)
        
        print(f"âœ… ì–‘ìí™” ì™„ë£Œ: {output_path}")
        return str(output_path)
        
    except Exception as e:
        print(f"âŒ ì–‘ìí™” ì‹¤íŒ¨: {e}")
        return None

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="ëª¨ë¸ ì–‘ìí™”")
    parser.add_argument("--model", required=True, help="ëª¨ë¸ ì´ë¦„")
    parser.add_argument("--output", required=True, help="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--bits", type=int, choices=[4, 8], default=8, help="ì–‘ìí™” ë¹„íŠ¸")
    
    args = parser.parse_args()
    
    quantize_model(args.model, args.output, args.bits)
"""
    
    with open(CACHE_DIR / "quantize_models.py", "w") as f:
        f.write(quantization_script)
    
    os.chmod(CACHE_DIR / "quantize_models.py", 0o755)
    print(f"âœ… ì–‘ìí™” ìŠ¤í¬ë¦½íŠ¸ ìƒì„±: {CACHE_DIR}/quantize_models.py")

def setup_memory_management():
    """ë©”ëª¨ë¦¬ ê´€ë¦¬ ì„¤ì •"""
    print("ğŸ§  ë©”ëª¨ë¦¬ ê´€ë¦¬ ì„¤ì •...")
    
    memory_script = """#!/usr/bin/env python3
'''
GPU ë©”ëª¨ë¦¬ ê´€ë¦¬ ìœ í‹¸ë¦¬í‹°
ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§, ìºì‹œ ì •ë¦¬, ìµœì í™”
'''

import torch
import gc
import psutil
import nvidia_ml_py3 as nvml
from typing import Dict, List

class GPUMemoryManager:
    def __init__(self):
        if torch.cuda.is_available():
            nvml.nvmlInit()
            self.device_count = torch.cuda.device_count()
        else:
            self.device_count = 0
    
    def get_memory_info(self) -> Dict:
        '''GPU ë©”ëª¨ë¦¬ ì •ë³´ ì¡°íšŒ'''
        if not torch.cuda.is_available():
            return {"error": "CUDA not available"}
        
        info = {}
        for i in range(self.device_count):
            handle = nvml.nvmlDeviceGetHandleByIndex(i)
            mem_info = nvml.nvmlDeviceGetMemoryInfo(handle)
            
            info[f"gpu_{i}"] = {
                "name": nvml.nvmlDeviceGetName(handle).decode(),
                "total_mb": round(mem_info.total / 1024**2),
                "used_mb": round(mem_info.used / 1024**2),
                "free_mb": round(mem_info.free / 1024**2),
                "utilization": round((mem_info.used / mem_info.total) * 100, 1)
            }
        
        return info
    
    def clear_cache(self):
        '''GPU ìºì‹œ ì •ë¦¬'''
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            gc.collect()
            print("âœ… GPU ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
        else:
            print("âš ï¸ CUDAê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤")
    
    def optimize_memory(self):
        '''ë©”ëª¨ë¦¬ ìµœì í™”'''
        # Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        gc.collect()
        
        # PyTorch ìºì‹œ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        
        print("âœ… ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ")
    
    def print_memory_summary(self):
        '''ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìš”ì•½ ì¶œë ¥'''
        print("ğŸ“Š ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:")
        
        # CPU ë©”ëª¨ë¦¬
        cpu_memory = psutil.virtual_memory()
        print(f"CPU: {cpu_memory.percent:.1f}% ({cpu_memory.used // 1024**3}GB / {cpu_memory.total // 1024**3}GB)")
        
        # GPU ë©”ëª¨ë¦¬
        gpu_info = self.get_memory_info()
        for gpu_id, info in gpu_info.items():
            if "error" not in info:
                print(f"{info['name']}: {info['utilization']:.1f}% ({info['used_mb']}MB / {info['total_mb']}MB)")

if __name__ == "__main__":
    manager = GPUMemoryManager()
    
    import sys
    if len(sys.argv) > 1:
        action = sys.argv[1]
        if action == "clear":
            manager.clear_cache()
        elif action == "optimize":
            manager.optimize_memory()
        elif action == "info":
            print(manager.get_memory_info())
        elif action == "summary":
            manager.print_memory_summary()
    else:
        manager.print_memory_summary()
"""
    
    with open(CACHE_DIR / "memory_manager.py", "w") as f:
        f.write(memory_script)
    
    os.chmod(CACHE_DIR / "memory_manager.py", 0o755)
    print(f"âœ… ë©”ëª¨ë¦¬ ê´€ë¦¬ì ìƒì„±: {CACHE_DIR}/memory_manager.py")

def setup_model_serving():
    """ëª¨ë¸ ì„œë¹™ ìµœì í™”"""
    print("ğŸš€ ëª¨ë¸ ì„œë¹™ ìµœì í™”...")
    
    serving_script = """#!/usr/bin/env python3
'''
ìµœì í™”ëœ ëª¨ë¸ ì„œë¹™ ì„œë²„
TensorRT, TorchScript, ONNX ë“± ìµœì í™” ì§€ì›
'''

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import torch
import asyncio
import json
from pathlib import Path
from typing import Optional, Dict, Any
import logging

app = FastAPI(title="Optimized Model Server")

class ModelRequest(BaseModel):
    text: str
    model_name: Optional[str] = "default"
    max_length: Optional[int] = 512
    temperature: Optional[float] = 0.7

class ModelManager:
    def __init__(self, config_path: str):
        self.models = {}
        self.config = self.load_config(config_path)
        
    def load_config(self, config_path: str) -> Dict:
        with open(config_path) as f:
            return json.load(f)
    
    async def load_model(self, model_name: str, optimization: str = "default"):
        '''ëª¨ë¸ ë¡œë“œ ë° ìµœì í™”'''
        if model_name in self.models:
            return self.models[model_name]
        
        config = self.config.get(optimization, self.config["default"])
        
        try:
            # ëª¨ë¸ ë¡œë“œ (ì‹¤ì œ êµ¬í˜„ í•„ìš”)
            print(f"ğŸ“¥ ëª¨ë¸ ë¡œë“œ ì¤‘: {model_name}")
            
            # ìµœì í™” ì ìš©
            if config["model_config"].get("torch_compile", False):
                # PyTorch 2.0 ì»´íŒŒì¼ ìµœì í™”
                pass
            
            self.models[model_name] = {
                "model": None,  # ì‹¤ì œ ëª¨ë¸ ê°ì²´
                "tokenizer": None,  # í† í¬ë‚˜ì´ì €
                "config": config
            }
            
            print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_name}")
            return self.models[model_name]
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise HTTPException(status_code=500, detail=str(e))

model_manager = ModelManager("/mnt/data/lfm_agi/cache/optimization_configs.json")

@app.post("/generate")
async def generate(request: ModelRequest):
    '''í…ìŠ¤íŠ¸ ìƒì„±'''
    try:
        model_info = await model_manager.load_model(request.model_name)
        
        # ì‹¤ì œ ìƒì„± ë¡œì§ (êµ¬í˜„ í•„ìš”)
        response = f"Generated response for: {request.text}"
        
        return {
            "response": response,
            "model": request.model_name,
            "parameters": {
                "max_length": request.max_length,
                "temperature": request.temperature
            }
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/models")
async def list_models():
    '''ë¡œë“œëœ ëª¨ë¸ ëª©ë¡'''
    return {"models": list(model_manager.models.keys())}

@app.get("/health")
async def health_check():
    '''í—¬ìŠ¤ ì²´í¬'''
    gpu_available = torch.cuda.is_available()
    gpu_count = torch.cuda.device_count() if gpu_available else 0
    
    return {
        "status": "healthy",
        "gpu_available": gpu_available,
        "gpu_count": gpu_count,
        "loaded_models": len(model_manager.models)
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001)
"""
    
    with open(CACHE_DIR / "model_server.py", "w") as f:
        f.write(serving_script)
    
    os.chmod(CACHE_DIR / "model_server.py", 0o755)
    print(f"âœ… ëª¨ë¸ ì„œë¹™ ì„œë²„ ìƒì„±: {CACHE_DIR}/model_server.py")

def create_monitoring_dashboard():
    """ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ ìƒì„±"""
    print("ğŸ“Š ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ ìƒì„±...")
    
    dashboard_html = """<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LFM AGI ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°</title>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; background: #1a1a2e; color: white; }
        .container { max-width: 1200px; margin: 0 auto; }
        .grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px; }
        .card { background: #16213e; padding: 20px; border-radius: 10px; box-shadow: 0 4px 6px rgba(0,0,0,0.3); }
        .metric { text-align: center; margin: 10px 0; }
        .metric-value { font-size: 2em; font-weight: bold; color: #4CAF50; }
        .metric-label { color: #ccc; }
        .chart-container { position: relative; height: 200px; margin: 20px 0; }
        .status { padding: 10px; border-radius: 5px; margin: 5px 0; }
        .status.online { background: #4CAF50; }
        .status.offline { background: #f44336; }
        button { background: #4CAF50; color: white; padding: 10px 20px; border: none; border-radius: 5px; cursor: pointer; }
        button:hover { background: #45a049; }
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸ¤– LFM AGI ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°</h1>
        
        <div class="grid">
            <div class="card">
                <h3>ğŸ–¥ï¸ ì‹œìŠ¤í…œ ì •ë³´</h3>
                <div class="metric">
                    <div class="metric-value" id="cpuUsage">--</div>
                    <div class="metric-label">CPU ì‚¬ìš©ë¥  (%)</div>
                </div>
                <div class="metric">
                    <div class="metric-value" id="memoryUsage">--</div>
                    <div class="metric-label">ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  (%)</div>
                </div>
                <div class="metric">
                    <div class="metric-value" id="diskUsage">--</div>
                    <div class="metric-label">ë””ìŠ¤í¬ ì‚¬ìš©ë¥  (%)</div>
                </div>
            </div>
            
            <div class="card">
                <h3>ğŸš€ GPU ì •ë³´</h3>
                <div id="gpuInfo"></div>
                <button onclick="clearGPUCache()">GPU ìºì‹œ ì •ë¦¬</button>
            </div>
            
            <div class="card">
                <h3>ğŸ¤– ëª¨ë¸ ìƒíƒœ</h3>
                <div id="modelStatus"></div>
                <button onclick="refreshModels()">ëª¨ë¸ ìƒˆë¡œê³ ì¹¨</button>
            </div>
            
            <div class="card">
                <h3>ğŸ“Š ì„±ëŠ¥ ì°¨íŠ¸</h3>
                <div class="chart-container">
                    <canvas id="performanceChart"></canvas>
                </div>
            </div>
        </div>
    </div>

    <script>
        // ì°¨íŠ¸ ì´ˆê¸°í™”
        const ctx = document.getElementById('performanceChart').getContext('2d');
        const chart = new Chart(ctx, {
            type: 'line',
            data: {
                labels: [],
                datasets: [{
                    label: 'GPU ì‚¬ìš©ë¥ ',
                    data: [],
                    borderColor: '#4CAF50',
                    tension: 0.1
                }]
            },
            options: {
                responsive: true,
                maintainAspectRatio: false,
                plugins: { legend: { labels: { color: 'white' } } },
                scales: {
                    y: { ticks: { color: 'white' } },
                    x: { ticks: { color: 'white' } }
                }
            }
        });

        // ë°ì´í„° ì—…ë°ì´íŠ¸ í•¨ìˆ˜ë“¤
        async function updateSystemInfo() {
            // ì‹¤ì œ API í˜¸ì¶œ ì‹œë®¬ë ˆì´ì…˜
            document.getElementById('cpuUsage').textContent = Math.floor(Math.random() * 100);
            document.getElementById('memoryUsage').textContent = Math.floor(Math.random() * 100);
            document.getElementById('diskUsage').textContent = Math.floor(Math.random() * 100);
        }

        async function updateGPUInfo() {
            const gpuDiv = document.getElementById('gpuInfo');
            gpuDiv.innerHTML = `
                <div class="metric">
                    <div class="metric-value">${Math.floor(Math.random() * 100)}</div>
                    <div class="metric-label">GPU 0 ì‚¬ìš©ë¥  (%)</div>
                </div>
                <div class="metric">
                    <div class="metric-value">${Math.floor(Math.random() * 100)}</div>
                    <div class="metric-label">GPU ë©”ëª¨ë¦¬ (%)</div>
                </div>
            `;
        }

        async function updateModelStatus() {
            const models = ['LFM-2.5-VL', 'EXAONE-3.5', 'KoAlpaca'];
            const statusDiv = document.getElementById('modelStatus');
            
            statusDiv.innerHTML = models.map(model => 
                `<div class="status online">${model}: ì˜¨ë¼ì¸</div>`
            ).join('');
        }

        async function clearGPUCache() {
            alert('GPU ìºì‹œë¥¼ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.');
        }

        async function refreshModels() {
            await updateModelStatus();
        }

        // ì •ê¸° ì—…ë°ì´íŠ¸
        setInterval(() => {
            updateSystemInfo();
            updateGPUInfo();
            updateModelStatus();
            
            // ì°¨íŠ¸ ì—…ë°ì´íŠ¸
            const now = new Date().toLocaleTimeString();
            chart.data.labels.push(now);
            chart.data.datasets[0].data.push(Math.floor(Math.random() * 100));
            
            if (chart.data.labels.length > 20) {
                chart.data.labels.shift();
                chart.data.datasets[0].data.shift();
            }
            
            chart.update();
        }, 2000);

        // ì´ˆê¸° ë¡œë“œ
        updateSystemInfo();
        updateGPUInfo();
        updateModelStatus();
    </script>
</body>
</html>"""
    
    with open(CACHE_DIR / "monitor.html", "w") as f:
        f.write(dashboard_html)
    
    print(f"âœ… ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ ìƒì„±: {CACHE_DIR}/monitor.html")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("âš¡ LFM AGI ìºì‹œ ë° ìµœì í™” ì„¤ì •")
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    CACHE_DIR.mkdir(exist_ok=True)
    
    # ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥
    system_info = get_system_info()
    print(f"\nğŸ’» ì‹œìŠ¤í…œ ì •ë³´:")
    print(f"CPU: {system_info['cpu_count']}ì½”ì–´")
    print(f"ë©”ëª¨ë¦¬: {system_info['memory_gb']}GB")
    print(f"ë””ìŠ¤í¬ ì—¬ìœ ê³µê°„: {system_info['disk_free_gb']}GB")
    print(f"CUDA: {system_info['cuda_available']}")
    
    if system_info['cuda_available']:
        for device in system_info['cuda_devices']:
            print(f"GPU {device['id']}: {device['name']} ({device['memory_gb']}GB)")
    
    print("\nì„¤ì • ì˜µì…˜:")
    print("1. ê¸°ë³¸ ìºì‹œ ì„¤ì •")
    print("2. ëª¨ë¸ ìµœì í™” ë„êµ¬")
    print("3. ë©”ëª¨ë¦¬ ê´€ë¦¬ ë„êµ¬") 
    print("4. ì„œë¹™ ì„œë²„ ì„¤ì •")
    print("5. ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ")
    print("6. ì „ì²´ ì„¤ì •")
    
    choice = input("ì„ íƒ (1-6): ").strip()
    
    if choice == "1":
        setup_environment_cache()
        create_optimization_configs()
        
    elif choice == "2":
        setup_model_quantization()
        
    elif choice == "3":
        setup_memory_management()
        
    elif choice == "4":
        setup_model_serving()
        
    elif choice == "5":
        create_monitoring_dashboard()
        
    elif choice == "6":
        setup_environment_cache()
        create_optimization_configs()
        setup_model_quantization()
        setup_memory_management()
        setup_model_serving()
        create_monitoring_dashboard()
        
    else:
        print("ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
        return
    
    print(f"\nğŸ‰ ìºì‹œ ë° ìµœì í™” ì„¤ì • ì™„ë£Œ!")
    print(f"ğŸ“ ìœ„ì¹˜: {CACHE_DIR}")
    print(f"ğŸ”§ í™˜ê²½ ì„¤ì •: source {BASE_DIR}/setup_env.sh")

if __name__ == "__main__":
    main()