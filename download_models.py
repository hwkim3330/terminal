#!/usr/bin/env python3
"""
LFM AGI ëª¨ë¸ ë° ë°ì´í„° ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸
"""

import os
import asyncio
import subprocess
from pathlib import Path
import requests
from tqdm import tqdm
import json

BASE_DIR = Path("/mnt/data/lfm_agi")
MODELS_DIR = BASE_DIR / "models"
DATASETS_DIR = BASE_DIR / "datasets"
CACHE_DIR = BASE_DIR / "huggingface_cache"
OLLAMA_DIR = BASE_DIR / "ollama_models"

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ["HF_HOME"] = str(CACHE_DIR)
os.environ["TRANSFORMERS_CACHE"] = str(CACHE_DIR)
os.environ["OLLAMA_MODELS"] = str(OLLAMA_DIR)

def download_file(url: str, filename: str):
    """íŒŒì¼ ë‹¤ìš´ë¡œë“œ with progress bar"""
    response = requests.get(url, stream=True)
    total_size = int(response.headers.get('content-length', 0))
    
    with open(filename, 'wb') as file, tqdm(
        desc=filename.name,
        total=total_size,
        unit='B',
        unit_scale=True
    ) as pbar:
        for chunk in response.iter_content(chunk_size=8192):
            if chunk:
                file.write(chunk)
                pbar.update(len(chunk))

def run_command(cmd: str):
    """ëª…ë ¹ì–´ ì‹¤í–‰"""
    print(f"ì‹¤í–‰ ì¤‘: {cmd}")
    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
    if result.returncode != 0:
        print(f"ì˜¤ë¥˜: {result.stderr}")
        return False
    print(f"ì™„ë£Œ: {cmd}")
    return True

class ModelDownloader:
    def __init__(self):
        self.downloaded_models = []
        
    def download_huggingface_models(self):
        """Hugging Face ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
        models = [
            # LFM ê³„ì—´ ëª¨ë¸
            "microsoft/DialoGPT-medium",
            "microsoft/DialoGPT-large", 
            
            # í•œêµ­ì–´ ëª¨ë¸
            "LGAI-EXAONE/EXAONE-3.5-32B-Instruct",
            "beomi/Llama-3-Open-Ko-8B",
            "beomi/KoAlpaca-Polyglot-5.8B",
            "klue/roberta-large",
            "klue/bert-base",
            
            # ë©€í‹°ëª¨ë‹¬ ëª¨ë¸
            "microsoft/kosmos-2-patch14-224",
            "Salesforce/blip2-opt-2.7b",
            "openai/clip-vit-large-patch14",
            
            # ì„ë² ë”© ëª¨ë¸
            "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            "jhgan/ko-sroberta-multitask",
            
            # ì½”ë“œ ëª¨ë¸
            "microsoft/CodeBERT-base",
            "bigcode/starcoder"
        ]
        
        print("ğŸ¤– Hugging Face ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
        
        for model in models:
            try:
                print(f"\nğŸ“¥ ë‹¤ìš´ë¡œë“œ ì¤‘: {model}")
                
                # git lfs ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ í´ë¡ 
                model_dir = MODELS_DIR / model.replace("/", "_")
                if not model_dir.exists():
                    cmd = f"cd {MODELS_DIR} && git lfs clone https://huggingface.co/{model} {model.replace('/', '_')}"
                    if run_command(cmd):
                        self.downloaded_models.append(model)
                else:
                    print(f"âœ… ì´ë¯¸ ì¡´ì¬: {model}")
                    
            except Exception as e:
                print(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ {model}: {e}")
        
    def download_ollama_models(self):
        """Ollama ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
        models = [
            "llama3.1:8b",
            "llama3.1:70b",
            "qwen2.5:7b",
            "qwen2.5:32b",
            "mistral:7b",
            "gemma2:9b",
            "phi3:medium",
            "codellama:7b",
            "codellama:34b"
        ]
        
        print("\nğŸ¦™ Ollama ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
        
        for model in models:
            try:
                print(f"\nğŸ“¥ ë‹¤ìš´ë¡œë“œ ì¤‘: {model}")
                cmd = f"OLLAMA_MODELS={OLLAMA_DIR} ollama pull {model}"
                if run_command(cmd):
                    self.downloaded_models.append(f"ollama:{model}")
                    
            except Exception as e:
                print(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ {model}: {e}")
    
    def download_datasets(self):
        """í•œêµ­ì–´ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ"""
        datasets = [
            # ëŒ€í™” ë°ì´í„°ì…‹
            {
                "name": "korean_conversation",
                "url": "https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv",
                "filename": "korean_conversation.csv"
            },
            
            # ë‰´ìŠ¤ ë°ì´í„°ì…‹
            {
                "name": "korean_news",
                "url": "https://raw.githubusercontent.com/lovit/korean_news_dataset/master/data/news_2018_sample.txt",
                "filename": "korean_news.txt"
            },
            
            # ê°ì • ë¶„ì„ ë°ì´í„°
            {
                "name": "korean_sentiment",
                "url": "https://raw.githubusercontent.com/park1200656/KnuSentiLex/master/Data/SentiWord_Dict.txt",
                "filename": "korean_sentiment.txt"
            }
        ]
        
        print("\nğŸ“Š í•œêµ­ì–´ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
        
        for dataset in datasets:
            try:
                filepath = DATASETS_DIR / dataset["filename"]
                if not filepath.exists():
                    print(f"\nğŸ“¥ ë‹¤ìš´ë¡œë“œ ì¤‘: {dataset['name']}")
                    download_file(dataset["url"], filepath)
                    print(f"âœ… ì™„ë£Œ: {dataset['name']}")
                else:
                    print(f"âœ… ì´ë¯¸ ì¡´ì¬: {dataset['name']}")
                    
            except Exception as e:
                print(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ {dataset['name']}: {e}")
    
    def setup_korean_nlp_resources(self):
        """í•œêµ­ì–´ NLP ë¦¬ì†ŒìŠ¤ ì„¤ì •"""
        print("\nğŸ‡°ğŸ‡· í•œêµ­ì–´ NLP ë¦¬ì†ŒìŠ¤ ì„¤ì •...")
        
        try:
            # KoNLPy ë°ì´í„° ë‹¤ìš´ë¡œë“œ
            import konlpy
            from konlpy.tag import Okt, Komoran, Hannanum, Mecab
            
            # í˜•íƒœì†Œ ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸
            okt = Okt()
            test_text = "ì•ˆë…•í•˜ì„¸ìš”. í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤."
            result = okt.morphs(test_text)
            print(f"âœ… KoNLPy ì„¤ì • ì™„ë£Œ: {result[:5]}")
            
        except Exception as e:
            print(f"âš ï¸ KoNLPy ì„¤ì • ì‹¤íŒ¨: {e}")
            print("pip install konlpy ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")
    
    def create_model_index(self):
        """ëª¨ë¸ ì¸ë±ìŠ¤ íŒŒì¼ ìƒì„±"""
        index = {
            "downloaded_at": str(datetime.now()),
            "models": {
                "huggingface": [],
                "ollama": [],
                "local": []
            },
            "datasets": [],
            "total_size_gb": 0
        }
        
        # ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ ìŠ¤ìº”
        for model_dir in MODELS_DIR.iterdir():
            if model_dir.is_dir():
                size = sum(f.stat().st_size for f in model_dir.rglob('*') if f.is_file())
                index["models"]["huggingface"].append({
                    "name": model_dir.name,
                    "path": str(model_dir),
                    "size_mb": round(size / (1024*1024), 2)
                })
        
        # Ollama ëª¨ë¸ ìŠ¤ìº”
        if OLLAMA_DIR.exists():
            for model_file in OLLAMA_DIR.rglob("*"):
                if model_file.is_file():
                    size = model_file.stat().st_size
                    index["models"]["ollama"].append({
                        "name": model_file.name,
                        "path": str(model_file),
                        "size_mb": round(size / (1024*1024), 2)
                    })
        
        # ë°ì´í„°ì…‹ ìŠ¤ìº”
        for dataset_file in DATASETS_DIR.iterdir():
            if dataset_file.is_file():
                size = dataset_file.stat().st_size
                index["datasets"].append({
                    "name": dataset_file.name,
                    "path": str(dataset_file),
                    "size_mb": round(size / (1024*1024), 2)
                })
        
        # ì´ í¬ê¸° ê³„ì‚°
        total_size = sum(model["size_mb"] for model in index["models"]["huggingface"])
        total_size += sum(model["size_mb"] for model in index["models"]["ollama"])
        total_size += sum(dataset["size_mb"] for dataset in index["datasets"])
        index["total_size_gb"] = round(total_size / 1024, 2)
        
        # ì¸ë±ìŠ¤ íŒŒì¼ ì €ì¥
        with open(BASE_DIR / "model_index.json", "w", encoding="utf-8") as f:
            json.dump(index, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“‹ ëª¨ë¸ ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ: {index['total_size_gb']} GB")
        return index

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ LFM AGI ëª¨ë¸ & ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì‹œì‘")
    print(f"ğŸ“‚ ê¸°ë³¸ ê²½ë¡œ: {BASE_DIR}")
    
    # ë””ë ‰í† ë¦¬ ìƒì„± í™•ì¸
    MODELS_DIR.mkdir(exist_ok=True)
    DATASETS_DIR.mkdir(exist_ok=True)
    CACHE_DIR.mkdir(exist_ok=True)
    OLLAMA_DIR.mkdir(exist_ok=True)
    
    downloader = ModelDownloader()
    
    # ì‚¬ìš©ì ì„ íƒ
    print("\në‹¤ìš´ë¡œë“œí•  í•­ëª©ì„ ì„ íƒí•˜ì„¸ìš”:")
    print("1. Hugging Face ëª¨ë¸")
    print("2. Ollama ëª¨ë¸")
    print("3. í•œêµ­ì–´ ë°ì´í„°ì…‹")
    print("4. ì „ì²´")
    print("5. ëª¨ë¸ ì¸ë±ìŠ¤ë§Œ ìƒì„±")
    
    choice = input("ì„ íƒ (1-5): ").strip()
    
    if choice == "1":
        downloader.download_huggingface_models()
    elif choice == "2":
        downloader.download_ollama_models()
    elif choice == "3":
        downloader.download_datasets()
        downloader.setup_korean_nlp_resources()
    elif choice == "4":
        downloader.download_huggingface_models()
        downloader.download_ollama_models()
        downloader.download_datasets()
        downloader.setup_korean_nlp_resources()
    elif choice == "5":
        pass
    else:
        print("ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
        return
    
    # ëª¨ë¸ ì¸ë±ìŠ¤ ìƒì„±
    from datetime import datetime
    index = downloader.create_model_index()
    
    print(f"\nğŸ‰ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
    print(f"ğŸ“Š ì´ í¬ê¸°: {index['total_size_gb']} GB")
    print(f"ğŸ“ ìœ„ì¹˜: {BASE_DIR}")

if __name__ == "__main__":
    main()